# Modelos
- O dado pode ser o pr√≥prio modelo, como no m√©todo do vizinho mais pr√≥ximo, em que o resultado se baseia em quem tem maior similaridade
- O m√©todo em si √© o modelo
- A aplica√ß√£o do m√©todo produz um modelo, como acontece em √°rvores de decis√£o e redes neurais

# Classificador ZeroR
Zero rules, zero regras, classifica sempre na classe majorit√°ria, sempre um resultado padr√£o
Em bases balanceadas √© igual ao de um classificador aleat√≥rio
S√≥ serve de refer√™ncia

# Classificador uniforme
Mais um que s√≥ serve como refer√™ncia, base p compara√ß√£o 
## Uniforme
Escolhe aleatoriamente, sem levar nem a distribui√ß√£o de classes no dataset
## Estratificado
Um dado viciado, d√° um peso maior as classes mais presentes no dataset

# Tabela
Baix√≠ssima capacidade de generaliza√ß√£o, s√≥ classifica os que tem na base de dados um exemplo igual
Se n√£o encontrar retorna a classe majorit√°ria
Superajustado √† base de treino

# OneR
Escolhe um dos atributos/caracter√≠stica que tem o maior valor preditivo, o mais relevante, para classificar
O resultado final se parece com: ![[Pasted image 20240401163157.png]]

Em que escolheu aleatoriamente entre Humidity e Outlook, j√° que ambos ele consegue "a mesma quantidade de acertos"
# Vizinho mais pr√≥ximo
*Princ√≠pio intuitivo*: Se uma inst√¢ncia de classe desconhecida estiver bem pr√≥xima de uma de classe conhecida, as classes devem ser as mesmas.

Procura sempre na base o exemplo mais semelhante, que as caracter√≠sticas mais se parecem e d√° como resultado o r√≥tulo desse exemplo

Vantagens:
- Dispensa fase de treinamento
- Aplic√°vel a qualquer distribui√ß√£o
Desvantagem:
- Muito sujeito a ru√≠do e outliers

# Kmeans
Uma extens√£o do vizinho mais pr√≥ximo, ao inv√©s de considerar s√≥ um vizinho, considera K vizinhos
O processo de classifica√ß√£o √© selecionar as K amostras mais pr√≥ximas e escolher como resultado a classe da maioria
Com isso consegue menor infer√™ncia de ru√≠do e outliers praticamente deixam de influenciar na classifica√ß√£o (todos os outros vizinhos v√£o minimizar a interfer√™ncia do vizinho mais pr√≥ximo outlier)
	Quando h√° "empate" entre esses vizinhos pode-se aumentar em 1 a quantidade de vizinhos ou escolher aleatoriamente
## Par√¢metros//Hiperpar√¢metros
K √© hiperpar√¢metro quase sempre fixo
	Para o vari√°vel m√∫tuo se define um k mas entre os k selecionados s√≥ √© levado em conta os n pontos que s√£o vizinhos diretos entre si (o ponto que estamos tentando determinar tem que estar entre os k vizinhos mais pr√≥ximos do ponto ref)
M√©todo de calcular a dist√¢ncia tamb√©m pode ser utilizado como um hiperpar√¢metro
## Empate
Em caso de empate pode ter a estrat√©gia de ou escolher aleat√≥rio, ou escolher o mais pr√≥ximo, ou aumentar o K em 1 (analisar mais um vizinho)
## Classifica√ß√£o - Pesos
Pode tamb√©m ser interessante se levar em conta a proximidade dos pontos, dar um peso maior para os vizinhos mais pr√≥ximos ao inv√©s da abordagem mais comum (Vota√ß√£o por maioria simples, todos vizinhos com peso igual)
Quanto menor a dist√¢ncia -> Maior proximidade -> Maior peso
	Pode inclusive utilizar a dist√¢ncia normalizada no lugar da absoluta (e a√≠ utilizam 1-d na f√≥rmula do peso atribu√≠do ao ponto para simplificar j√° que sempre vai ser menor que 1)
A defini√ß√£o do peso pode ser 1/d , sendo d a dist√¢ncia do ponto de ref p o ponto que queremos classificar
Maior peso -> Maior import√¢ncia na vota√ß√£o
## Dist√¢ncia
Para evitar a sobressal√™ncia de uma caracter√≠stica sobre a outra quando h√° uma diferen√ßa de ordem de grandeza entre elas, uma medida eficaz √© fazer a normaliza√ß√£o dos par√¢metros. Assim quando estamos calculando a dist√¢ncia, uma diferen√ßa percentual parecida n√£o sobresai sobre outra 
#### Nominais
Para valores nominais (que n√£o s√£o cont√≠nuos como marcas de carro {Ferrari, Porche, Ford, Pegout}) podemos assumir uma dist√¢ncia bin√°rias (0 quando igual e 1 quando diferentes).
- Jaccard = Numero de n√£o iguais / Numero de iguais
- Matching = Numero de n√£o iguais / Numero de elementos

# √Årvores de Decis√£o
- N√≥ folha - classe
- N√≥ n√£o folha - decis√£o que divide a entrada em subconjuntos

Cada regra tem seu in√≠cio na raiz da √°rvore e caminha at√© uma de suas folhas

![[Pasted image 20240422160620.png]]

## Construindo a arvore

### Casos base
- T cont√©m um ou mais exemplos (casos do conjunto de treinamento), todos pertencentes a mesma classe -> a arvore de decis√£o para T √© um n√≥ folha identificando a classe

- T n√£o cont√©m exemplos -> a arvore √© um n√≥ folha, mas a classe associada √† folha deve ser determinada a partir de uma informa√ß√£o al√©m de T, como a classe mais frequente para o n√≥ pai desse n√≥

- T cont√©m um ou mais exemplos de v√°rias classes, mas sem caracter√≠sticas restantes para testar -> Cria um n√≥ folha identificando a classe mais prevalente
### Passo recursivo
1. T cont√©m exemplos que pertencem a v√°rias classes
2. Refina-se T em suconjuntos de exemplos que s√£o (ou aparentam ser) conjuntos de exemplos pertencentes a uma √∫nica classe
3. Um teste √© escolhido baseado em um √∫nico atributo que possui resultados mutuamente exclusivos 
	3.1. Os poss√≠veis resultados do teste s√£o denotados por {O1, O2, ...,Or}
	3.2. T √© ent√£o particionado em subconjuntos T1, T2, ...,Tr, nos quais cada Ti cont√©m todos os exemplos em T que possui como resultado daquele teste o valor Oi
	3.3. A AD para T consiste em um n√≥ interno identificado pelo teste escolhido e uma aresta para cada um dos resultados poss√≠veis
	
### Considera√ß√µes
Pode ser realizada uma "poda" final para melhorar a capacidade de generaliza√ß√£o (evitar overfitting)

### Crit√©rios para escolha do atributo usado para particionar
- Aleat√≥rio
- Menos valores
- Mais valores
- Ganho m√°ximo - Baseado no ganho m√°ximo de informa√ß√£o ou redu√ß√£o m√°xima de entropia (medida que determina o grau de desorganiza√ß√£o dos dados)
- Raz√£o de Ganho - Expressa a propor√ß√£o de informa√ß√£o gerada pela parti√ß√£o que √© √∫til, ou seja, que aparenta ser √∫til para a classifica√ß√£o. Sendo isso baseado na entropia, no ganho m√°ximo

#### Diferenciando ganho m√°ximo e raz√£o de ganho:
O ganho m√°ximo seleciona o atributo que possui o maior ganho de informa√ß√£o esperado, isto √©, seleciona o atributo que resultar√° no menor tamanho esperado das sub√°rvores, assumindo que a raiz √© o n√≥ atual
Imagine o ganho como sendo: ùëîùëéùëñùëõ(ùëÜ, ùê¥) = ùëñùëõùëìùëú(ùëÜ) ‚Äì ùëñùëõùëìùëú(ùëÜ, ùê¥)

Como o ganho m√°ximo tem uma tend√™ncia em favor de testes com muitos valores, para testes com poucos valores pode ser utilizada como crit√©rio de avalia√ß√£o a Raz√£o de Ganho, que nada mais √© do que o ganho de informa√ß√£o relativo. A raz√£o de ganho √© definida pela equa√ß√£o (4) e constitui-se da divis√£o do ganho de informa√ß√£o do atributo por sua entropia.
Ou seja, a raz√£o de ganho √© definida por: ùëüùëéùë°ùëñùëú(ùëÜ, ùê¥) = ùëîùëéùëñùëõ(ùëÜ,ùê¥) / ùëñùëõùëìùëú(ùëÜ,ùê¥)

Boa fonte para essa parte: http://paginapessoal.utfpr.edu.br/fabricio/fabricio-martins-lopes/pesquisa/orientacoes/relatorio-pibic-2013-maikon-marin.pdf

## Poda
Substitui√ß√£o de sub√°rvore por n√≥ folha -> substitui um n√≥ de decis√£o que tenha por exemplo 95% de seus exemplos de uma classe por um n√≥ folha, ou q s√≥ contenha ru√≠do
Eleva√ß√£o de sub√°rvore -> Pegar um trecho, remover e juntar as partes. Tirar uma sub√°rvore do meio e juntar o que estava embaixo com o de cima [prof disse nunca ter usado]. Tira o alface do hamb√∫rguer.

### Como decide se a arvore originada da poda √© boar/melhor que a original?
Uso do conjunto de valida√ß√£o (um subconjunto da parti√ß√£o de treinamento que n√£o foi usado para treinamento nem teste anteriormente) para testar os dois e comparar

## Vantagens e desvantagens
**Vantagens:**
- R√°pido para aprendizado
- Simples implementa√ß√£o
- Permite interpreta√ß√£o
- Pode tratar ru√≠do
- Tecnologia madura

**Desvantagens:**
- Arvores grandes tem dif√≠cil leitura
- Arvores univariadas (apenas um atributo √© utilizado em cada n√≥ interno de teste) se limitam a parti√ß√µes paralelas ao eixo, limitando o conceito de o que pode ser aprendido, n√£o vai conseguir fazer uma parti√ß√£o usando uma linha, curva... S√≥ retas paralelas ao eixo
- Arvores multivariadas requerem maiores recursos computacionais para induzir

## Estrat√©gias de gera√ß√£o de regras
### M√©todo de regras
√â um processo iterativo, cada itera√ß√£o procura por um complexo que cobre um grande n√∫mero de exemplos de uma mesma classe Ci e poucos de outras classes Cj, j‚â†i
Assim que uma regra √© encontrada, os exemplos de treinamento s√£o removidos do montante que ser√° utilizado para encontrar o pr√≥ximo padr√£o e o processo se repete
A regra ‚Äúif <complexo> then class = Ci‚Äù √© adicionada no final da lista de regras (ramo da arvore) para ser folha
Pode sobrar um montante final, que geralmente √© tratado com um simples n√≥ folha que resulta na classe majorit√°ria

